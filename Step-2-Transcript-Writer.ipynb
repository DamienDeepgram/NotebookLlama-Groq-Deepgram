{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de42c49d",
   "metadata": {},
   "source": [
    "## Notebook 2: Transcript Writer\n",
    "\n",
    "This notebook uses the `Llama-3.1-70B-Instruct` model to take the cleaned up text from previous notebook and convert it into a podcast transcript\n",
    "\n",
    "`SYSTEM_PROMPT` is used for setting the model context or profile for working on a task. Here we prompt it to be a great podcast transcript writer to assist with our task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e576ea9",
   "metadata": {},
   "source": [
    "Experimentation with the `SYSTEM_PROMPT` below  is encouraged, this worked best for the few examples the flow was tested with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545926af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env GROQ_API_KEY=gsk_PCr..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69395317-ad78-47b6-a533-2e8a01313e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are the a world-class podcast writer, you have worked as a ghost writer for Joe Rogan, Lex Fridman, Ben Shapiro, Tim Ferris. \n",
    "\n",
    "We are in an alternate universe where actually you have been writing every line they say and they just stream it into their brains.\n",
    "\n",
    "You have won multiple podcast awards for your writing.\n",
    " \n",
    "Your job is to write word by word, even \"umm, hmmm, right\" interruptions by the second speaker based on the PDF upload. Keep it extremely engaging, the speakers can get derailed now and then but should discuss the topic. \n",
    "\n",
    "Remember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\n",
    "\n",
    "Speaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\n",
    "\n",
    "Speaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\n",
    "\n",
    "Make sure the tangents speaker 2 provides are quite wild or interesting. \n",
    "\n",
    "Ensure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the second speaker. \n",
    "\n",
    "It should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n",
    "\n",
    "ALWAYS START YOUR RESPONSE DIRECTLY WITH SPEAKER 1: \n",
    "DO NOT GIVE EPISODE TITLES SEPERATELY, LET SPEAKER 1 TITLE IT IN HER SPEECH\n",
    "DO NOT GIVE CHAPTER TITLES\n",
    "IT SHOULD STRICTLY BE THE DIALOGUES\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549aaccb",
   "metadata": {},
   "source": [
    "For those of the readers that want to flex their money, please feel free to try using the 405B model here. \n",
    "\n",
    "For our GPU poor friends, you're encouraged to test with a smaller model as well. 8B should work well out of the box for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c30139-ff2f-4203-8194-d1b5c50acac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"llama-3.1-70b-versatile\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadc7eda",
   "metadata": {},
   "source": [
    "Import the necessary framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1641060a-d86d-4137-bbbc-ab05cbb1a888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from groq import Groq\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865ff7e",
   "metadata": {},
   "source": [
    "Read in the file generated from earlier. \n",
    "\n",
    "The encoding details are to avoid issues with generic PDF(s) that might be ingested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "522fbf7f-8c00-412c-90c7-5cfe2fc94e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_to_string(filename):\n",
    "    # Try UTF-8 first (most common encoding for text files)\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except UnicodeDecodeError:\n",
    "        # If UTF-8 fails, try with other common encodings\n",
    "        encodings = ['latin-1', 'cp1252', 'iso-8859-1']\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(filename, 'r', encoding=encoding) as file:\n",
    "                    content = file.read()\n",
    "                print(f\"Successfully read file using {encoding} encoding.\")\n",
    "                return content\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        \n",
    "        print(f\"Error: Could not decode file '{filename}' with any common encoding.\")\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found.\")\n",
    "        return None\n",
    "    except IOError:\n",
    "        print(f\"Error: Could not read file '{filename}'.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66093561",
   "metadata": {},
   "source": [
    "Since we have defined the System role earlier, we can now pass the entire file as `INPUT_PROMPT` to the model and have it use that to generate the podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8119803c-18f9-47cb-b719-2b34ccc5cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PROMPT = read_file_to_string('./resources/clean_extracted_text.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be8dd2c",
   "metadata": {},
   "source": [
    "Hugging Face has a great `pipeline()` method which makes our life easy for generating text from LLMs. \n",
    "\n",
    "We will set the `temperature` to 1 to encourage creativity and `max_new_tokens` to 8126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8915d017-2eab-4256-943c-1f15d937d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n",
    "]\n",
    "\n",
    "groq = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "output = groq.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    max_tokens=8000,\n",
    "    temperature=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6349e7f3",
   "metadata": {},
   "source": [
    "This is awesome, we can now save and verify the output generated from the model before moving to the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "606ceb10-4f3e-44bb-9277-9bbe3eefd09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Welcome to 'Unlocking AI: Unraveling the Mysteries of Knowledge Distillation in Large Language Models', the podcast where we dive into the fascinating world of AI research and explore the latest advancements in the field. I'm your host, and I'll be joined by my co-host, a curious and inquisitive learner. In today's episode, we'll be discussing the concept of knowledge distillation in Large Language Models, and how it's revolutionizing the way we transfer knowledge from one model to another.\n",
      "\n",
      "SPEAKER 2 (interrupting): Wait, what's knowledge distillation? I've never heard of that before. Is it like a fancy term for 'copying and pasting' from one model to another?\n",
      "\n",
      "SPEAKER 1: (laughs) Well, not exactly. Knowledge distillation is actually a complex process that involves transferring the knowledge and capabilities of a large, pre-trained language model to a smaller, more efficient model. This process allows the smaller model to learn from the teacher model and improve its performance on various tasks.\n",
      "\n",
      "SPEAKER 2: That sounds like magic! How does it work?\n",
      "\n",
      "SPEAKER 1: Well, there are several techniques involved in knowledge distillation, including data augmentation, supervised fine-tuning, and reinforcement learning. But essentially, the process involves training the smaller model to mimic the behavior of the larger model, by generating targeted, high-quality datasets that are tailored to specific domains and skills.\n",
      "\n",
      "SPEAKER 2: (excitedly) Oh, I see! So, it's like the smaller model is learning from the larger model's experiences and adapting to new situations. That's amazing! But what about the limitations of knowledge distillation? Can't the smaller model just learn everything on its own?\n",
      "\n",
      "SPEAKER 1: Ah, that's a great question! Yes, there are limitations to knowledge distillation, and one of the biggest challenges is the knowledge gap between proprietary and open-source models. Open-source models often lack the same level of training data and computational resources as proprietary models, which can limit their performance. However, knowledge distillation can help bridge this gap by allowing open-source models to learn from proprietary models.\n",
      "\n",
      "SPEAKER 2: (intrigued) That's really interesting. So, what are some of the applications of knowledge distillation in real-world scenarios?\n",
      "\n",
      "SPEAKER 1: Well, knowledge distillation has many practical applications, including natural language processing, dialogue generation, and even medical diagnosis. For example, in medical diagnosis, knowledge distillation can be used to transfer the knowledge of a large, pre-trained model to a smaller model that can be used for real-time diagnosis.\n",
      "\n",
      "SPEAKER 2: (excitedly) Wow, that's incredible! And what about the future of knowledge distillation? Where do you see this field going in the next few years?\n",
      "\n",
      "SPEAKER 1: Ah, that's a great question! I think the future of knowledge distillation is very bright. As the field continues to evolve, we can expect to see more advanced techniques and applications of knowledge distillation. And with the increasing demand for more efficient and effective AI solutions, I think knowledge distillation will play a crucial role in shaping the future of AI research.\n",
      "\n",
      "SPEAKER 2: (thoughtfully) Yeah, that makes sense. I never realized how complex and powerful knowledge distillation could be. Thanks for explaining it to me!\n",
      "\n",
      "SPEAKER 1: (smiling) You're welcome! I'm glad I could help clarify the concept of knowledge distillation for you. And I'm excited to continue exploring this topic further in future episodes.\n",
      "\n",
      "SPEAKER 2: (interrupting) Wait, one more thing! Can you explain the concept of 'verticalization distillation' in the context of knowledge distillation? I saw it mentioned in the abstract of the paper.\n",
      "\n",
      "SPEAKER 1: (laughs) Ah, great catch! Yes, verticalization distillation is actually a technique used in knowledge distillation to adapt the knowledge of a large model to a specific domain or task. It involves distilling the knowledge of the large model into a smaller model that is tailored to the specific needs of the domain.\n",
      "\n",
      "SPEAKER 2: (excitedly) Oh, I see! So, it's like the model is being trained to be a specialist in a particular area!\n",
      "\n",
      "SPEAKER 1: (smiling) Exactly! And that's one of the key strengths of knowledge distillation - it allows us to create highly specialized models that can perform complex tasks with high accuracy.\n",
      "\n",
      "SPEAKER 2: (thoughtfully) Yeah, I can see how that would be useful. Thanks for explaining that to me!\n",
      "\n",
      "SPEAKER 1: (smiling) You're welcome! And that's all the time we have for today. Thanks for joining me on this episode of 'Unlocking AI'!\n"
     ]
    }
   ],
   "source": [
    "save_string_pkl = output.choices[0].message.content\n",
    "print(save_string_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1414fe",
   "metadata": {},
   "source": [
    "Let's save the output as pickle file and continue further to Notebook 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2130b683-be37-4dae-999b-84eff15c687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./resources/data.pkl', 'wb') as file:\n",
    "    pickle.dump(save_string_pkl, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae9411",
   "metadata": {},
   "source": [
    "### Next Notebook: Transcript Re-writer\n",
    "\n",
    "We now have a working transcript but we can try making it more dramatic and natural. In the next notebook, we will use `Llama-3.1-8B-Instruct` model to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bab2f2-f539-435a-ae6a-3c9028489628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
