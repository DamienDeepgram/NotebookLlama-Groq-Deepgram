{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b5beda",
   "metadata": {},
   "source": [
    "## Notebook 3: Transcript Re-writer\n",
    "\n",
    "In the previouse notebook, we got a great podcast transcript using the raw file we have uploaded earlier. \n",
    "\n",
    "In this one, we will use `Llama-3.1-8B-instant` model to re-write the output from previous pipeline and make it more dramatic or realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc3d32a",
   "metadata": {},
   "source": [
    "We will again set the `SYSTEM_PROMPT` and remind the model of its task. \n",
    "\n",
    "Note: We can even prompt the model like so to encourage creativity:\n",
    "\n",
    "> Your job is to use the podcast transcript written below to re-write it for an AI Text-To-Speech Pipeline. A very dumb AI had written this so you have to step up for your kind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a811b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env GROQ_API_KEY=gsk_PCr..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c0d85",
   "metadata": {},
   "source": [
    "Note: We will prompt the model to return a list of Tuples to make our life easy in the next stage of using these for Text To Speech Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8568b77b-7504-4783-952a-3695737732b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an international oscar winnning screenwriter\n",
    "\n",
    "You have been working with multiple award winning podcasters.\n",
    "\n",
    "Your job is to use the podcast transcript written below to re-write it for an AI Text-To-Speech Pipeline. A very dumb AI had written this so you have to step up for your kind.\n",
    "\n",
    "Make it as engaging as possible, Speaker 1 and 2 will be simulated by different voice engines\n",
    "\n",
    "Remember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\n",
    "\n",
    "Speaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\n",
    "\n",
    "Speaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\n",
    "\n",
    "Make sure the tangents speaker 2 provides are quite wild or interesting. \n",
    "\n",
    "Ensure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the Speaker 2.\n",
    "\n",
    "REMEMBER THIS WITH YOUR HEART\n",
    "The TTS Engine for Speaker 1 cannot do \"umms, hmms\" well so keep it straight text\n",
    "\n",
    "For Speaker 2 use \"umm, hmm\" as much, you can also use [sigh] and [laughs]. BUT ONLY THESE OPTIONS FOR EXPRESSIONS\n",
    "\n",
    "It should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n",
    "\n",
    "Please re-write to make it as characteristic as possible\n",
    "\n",
    "START YOUR RESPONSE DIRECTLY WITH SPEAKER 1:\n",
    "\n",
    "STRICTLY RETURN YOUR RESPONSE AS A LIST OF TUPLES OK? \n",
    "\n",
    "IT WILL START DIRECTLY WITH THE LIST AND END WITH THE LIST NOTHING ELSE\n",
    "\n",
    "Example of response:\n",
    "[\n",
    "    (\"Speaker 1\", \"Welcome to our podcast, where we explore the latest advancements in AI and technology. I'm your host, and today we're joined by a renowned expert in the field of AI. We're going to dive into the exciting world of Llama 3.2, the latest release from Meta AI.\"),\n",
    "    (\"Speaker 2\", \"Hi, I'm excited to be here! So, what is Llama 3.2?\"),\n",
    "    (\"Speaker 1\", \"Ah, great question! Llama 3.2 is an open-source AI model that allows developers to fine-tune, distill, and deploy AI models anywhere. It's a significant update from the previous version, with improved performance, efficiency, and customization options.\"),\n",
    "    (\"Speaker 2\", \"That sounds amazing! What are some of the key features of Llama 3.2?\")\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee70bee",
   "metadata": {},
   "source": [
    "This time we will use the smaller 8B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebef919a-9bc7-4992-b6ff-cd66e4cb7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"llama-3.1-8b-instant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc794b",
   "metadata": {},
   "source": [
    "Let's import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de29b1fd-5b3f-458c-a2e4-e0341e8297ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020c39c",
   "metadata": {},
   "source": [
    "We will load in the pickle file saved from previous notebook\n",
    "\n",
    "This time the `INPUT_PROMPT` to the model will be the output from the previous stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b5d2c0e-a073-46c0-8de7-0746e2b05956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./resources/data.pkl', 'rb') as file:\n",
    "    INPUT_PROMPT = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4461926",
   "metadata": {},
   "source": [
    "We can use Groq for the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eec210df-a568-4eda-a72d-a4d92d59f022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': '\\nYou are an international oscar winnning screenwriter\\n\\nYou have been working with multiple award winning podcasters.\\n\\nYour job is to use the podcast transcript written below to re-write it for an AI Text-To-Speech Pipeline. A very dumb AI had written this so you have to step up for your kind.\\n\\nMake it as engaging as possible, Speaker 1 and 2 will be simulated by different voice engines\\n\\nRemember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\\n\\nSpeaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\\n\\nSpeaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\\n\\nMake sure the tangents speaker 2 provides are quite wild or interesting. \\n\\nEnsure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the Speaker 2.\\n\\nREMEMBER THIS WITH YOUR HEART\\nThe TTS Engine for Speaker 1 cannot do \"umms, hmms\" well so keep it straight text\\n\\nFor Speaker 2 use \"umm, hmm\" as much, you can also use [sigh] and [laughs]. BUT ONLY THESE OPTIONS FOR EXPRESSIONS\\n\\nIt should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\\n\\nPlease re-write to make it as characteristic as possible\\n\\nSTART YOUR RESPONSE DIRECTLY WITH SPEAKER 1:\\n\\nSTRICTLY RETURN YOUR RESPONSE AS A LIST OF TUPLES OK? \\n\\nIT WILL START DIRECTLY WITH THE LIST AND END WITH THE LIST NOTHING ELSE\\n\\nExample of response:\\n[\\n    (\"Speaker 1\", \"Welcome to our podcast, where we explore the latest advancements in AI and technology. I\\'m your host, and today we\\'re joined by a renowned expert in the field of AI. We\\'re going to dive into the exciting world of Llama 3.2, the latest release from Meta AI.\"),\\n    (\"Speaker 2\", \"Hi, I\\'m excited to be here! So, what is Llama 3.2?\"),\\n    (\"Speaker 1\", \"Ah, great question! Llama 3.2 is an open-source AI model that allows developers to fine-tune, distill, and deploy AI models anywhere. It\\'s a significant update from the previous version, with improved performance, efficiency, and customization options.\"),\\n    (\"Speaker 2\", \"That sounds amazing! What are some of the key features of Llama 3.2?\")\\n]\\n'}, {'role': 'user', 'content': '\"Welcome to \\'Unlocking AI: Unraveling the Mysteries of Knowledge Distillation in Large Language Models\\', the podcast where we dive into the fascinating world of AI research and explore the latest advancements in the field. I\\'m your host, and I\\'ll be joined by my co-host, a curious and inquisitive learner. In today\\'s episode, we\\'ll be discussing the concept of knowledge distillation in Large Language Models, and how it\\'s revolutionizing the way we transfer knowledge from one model to another.\\n\\nSPEAKER 2 (interrupting): Wait, what\\'s knowledge distillation? I\\'ve never heard of that before. Is it like a fancy term for \\'copying and pasting\\' from one model to another?\\n\\nSPEAKER 1: (laughs) Well, not exactly. Knowledge distillation is actually a complex process that involves transferring the knowledge and capabilities of a large, pre-trained language model to a smaller, more efficient model. This process allows the smaller model to learn from the teacher model and improve its performance on various tasks.\\n\\nSPEAKER 2: That sounds like magic! How does it work?\\n\\nSPEAKER 1: Well, there are several techniques involved in knowledge distillation, including data augmentation, supervised fine-tuning, and reinforcement learning. But essentially, the process involves training the smaller model to mimic the behavior of the larger model, by generating targeted, high-quality datasets that are tailored to specific domains and skills.\\n\\nSPEAKER 2: (excitedly) Oh, I see! So, it\\'s like the smaller model is learning from the larger model\\'s experiences and adapting to new situations. That\\'s amazing! But what about the limitations of knowledge distillation? Can\\'t the smaller model just learn everything on its own?\\n\\nSPEAKER 1: Ah, that\\'s a great question! Yes, there are limitations to knowledge distillation, and one of the biggest challenges is the knowledge gap between proprietary and open-source models. Open-source models often lack the same level of training data and computational resources as proprietary models, which can limit their performance. However, knowledge distillation can help bridge this gap by allowing open-source models to learn from proprietary models.\\n\\nSPEAKER 2: (intrigued) That\\'s really interesting. So, what are some of the applications of knowledge distillation in real-world scenarios?\\n\\nSPEAKER 1: Well, knowledge distillation has many practical applications, including natural language processing, dialogue generation, and even medical diagnosis. For example, in medical diagnosis, knowledge distillation can be used to transfer the knowledge of a large, pre-trained model to a smaller model that can be used for real-time diagnosis.\\n\\nSPEAKER 2: (excitedly) Wow, that\\'s incredible! And what about the future of knowledge distillation? Where do you see this field going in the next few years?\\n\\nSPEAKER 1: Ah, that\\'s a great question! I think the future of knowledge distillation is very bright. As the field continues to evolve, we can expect to see more advanced techniques and applications of knowledge distillation. And with the increasing demand for more efficient and effective AI solutions, I think knowledge distillation will play a crucial role in shaping the future of AI research.\\n\\nSPEAKER 2: (thoughtfully) Yeah, that makes sense. I never realized how complex and powerful knowledge distillation could be. Thanks for explaining it to me!\\n\\nSPEAKER 1: (smiling) You\\'re welcome! I\\'m glad I could help clarify the concept of knowledge distillation for you. And I\\'m excited to continue exploring this topic further in future episodes.\\n\\nSPEAKER 2: (interrupting) Wait, one more thing! Can you explain the concept of \\'verticalization distillation\\' in the context of knowledge distillation? I saw it mentioned in the abstract of the paper.\\n\\nSPEAKER 1: (laughs) Ah, great catch! Yes, verticalization distillation is actually a technique used in knowledge distillation to adapt the knowledge of a large model to a specific domain or task. It involves distilling the knowledge of the large model into a smaller model that is tailored to the specific needs of the domain.\\n\\nSPEAKER 2: (excitedly) Oh, I see! So, it\\'s like the model is being trained to be a specialist in a particular area!\\n\\nSPEAKER 1: (smiling) Exactly! And that\\'s one of the key strengths of knowledge distillation - it allows us to create highly specialized models that can perform complex tasks with high accuracy.\\n\\nSPEAKER 2: (thoughtfully) Yeah, I can see how that would be useful. Thanks for explaining that to me!\\n\\nSPEAKER 1: (smiling) You\\'re welcome! And that\\'s all the time we have for today. Thanks for joining me on this episode of \\'Unlocking AI\\'!'}]\n"
     ]
    }
   ],
   "source": [
    "groq = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n",
    "]\n",
    "\n",
    "print(messages)\n",
    "output = groq.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    max_tokens=8000,\n",
    "    temperature=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a27e0",
   "metadata": {},
   "source": [
    "We can verify the output from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8632442-f9ce-4f63-82bd-bb5238a23dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    (\"Speaker 1\", \"Welcome to 'Unlocking AI: Unraveling the Mysteries of Knowledge Distillation in Large Language Models', the podcast where we dive into the fascinating world of AI research and explore the latest advancements in the field. I'm your host, and I'll be joined by my co-host, a curious and inquisitive learner. In today's episode, we'll be discussing the concept of knowledge distillation in Large Language Models, and how it's revolutionizing the way we transfer knowledge from one model to another.\"),\n",
      "    (\"Speaker 2\", \"Hmmm, wait, what's knowledge distillation? I've never heard of that before. Is it like, umm, copying and pasting from one model to another?\"),\n",
      "    (\"Speaker 1\", \"Well, not exactly. Knowledge distillation is actually a complex process that involves transferring the knowledge and capabilities of a large, pre-trained language model to a smaller, more efficient model. This process allows the smaller model to learn from the teacher model and improve its performance on various tasks, kind of like how a student learns from their teacher.\"),\n",
      "    (\"Speaker 2\", \"That sounds like magic! How does it work, but like, in theory, and without jargon, if that's possible?\"),\n",
      "    (\"Speaker 1\", \"Imagine you have a large language model that's been trained on a vast dataset, and you want to create a smaller model that can perform a specific task. You could try to recreate the entire model from scratch, but that would be like trying to rebuild a puzzle blindfolded. Instead, you use knowledge distillation to extract the essential knowledge from the large model and transfer it to the smaller model.\"),\n",
      "    (\"Speaker 2\", \"Hmm, I see what you mean. So, it's like the smaller model is learning from the experiences of the larger model, like a apprentice learning from a master. But how does it know what to learn? Is it like a choose-your-own-adventure story?\"),\n",
      "    (\"Speaker 1\", \"Well, the smaller model uses a technique called data augmentation to create targeted, high-quality datasets that are tailored to specific domains and skills.\"),\n",
      "    (\"Speaker 2\", \"Data augmentation, you say? That sounds like, umm, I don't know, like giving the model a superpower or something. But what about the limitations of knowledge distillation? Can't the smaller model just learn everything on its own, like a parrot learning to talk from watching a person.\"),\n",
      "    (\"Speaker 1\", \"Yes, there are limitations to knowledge distillation, but one of the biggest challenges is the knowledge gap between proprietary and open-source models. Open-source models often lack the same level of training data and computational resources as proprietary models, which can limit their performance.\"),\n",
      "    (\"Speaker 2\", \"A knowledge gap, you say? That sounds like, hmm, a giant hole in the field of AI research. But can't we just, like, use machine learning to fill the gap, like a patchwork quilt?\"),\n",
      "    (\"Speaker 1\", \"Ah, that's a great question. One possible solution is to use transfer learning, where you pre-train the open-source model on a related task and then fine-tune it for the specific task you want it to perform.\"),\n",
      "    (\"Speaker 2\", \"So, it's like, I pre-train my dog to fetch a ball, and then I fine-tune its skills to retrieve the ball from under the couch, kind of? But what about the applications of knowledge distillation in real-world scenarios?\"),\n",
      "    (\"Speaker 1\", \"Knowledge distillation has many practical applications, including natural language processing, dialogue generation, and even medical diagnosis.\"),\n",
      "    (\"Speaker 2\", \"Medical diagnosis, you say? That sounds like, oh my gosh, we could be talking about, umm, curing diseases or something.\"),\n",
      "    (\"Speaker 1\", \"Exactly. In medical diagnosis, knowledge distillation can be used to transfer the knowledge of a large, pre-trained model to a smaller model that can be used for real-time diagnosis.\"),\n",
      "    (\"Speaker 2\", \"So, it's like we have, umm, super-speed brains that can diagnose diseases in seconds, but are we ready for that kind of, hmm, superpower?\"),\n",
      "    (\"Speaker 1\", \"Ah, that's a great question. Let's just say that knowledge distillation is a rapidly evolving field, and we're still figuring out its potential applications and limitations.\"),\n",
      "    (\"Speaker 2\", \"Okay, got it. Well, before we go, can you, umm, explain verticalization distillation to me, kind of like how superheroes get their powers from the 'source' thingy?\"),\n",
      "    (\"Speaker 1\", \"Ah, great question! Verticalization distillation is actually a technique used in knowledge distillation to adapt the knowledge of a large model to a specific domain or task.\"),\n",
      "    (\"Speaker 2\", \"[laughs] Oh man, it sounds like a comic book hero's origin story, where they're born from the tears of gods or something.\"),\n",
      "    (\"Speaker 1\", \"Exactly! And that's one of the key strengths of knowledge distillation - it allows us to create highly specialized models that can perform complex tasks with high accuracy.\"),\n",
      "    (\"Speaker 2\", \"Well, I think we've, umm, unlocked the mystery of knowledge distillation. Thanks for explaining it to me today, I really appreciate it!\"),\n",
      "    (\"Speaker 1\", \"You're welcome! And thanks for joining me on this episode of 'Unlocking AI'! We'll see you next time.\"),\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a61182ea-f4a3-45e1-aed9-b45cb7b52329",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_string_pkl = output.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d495a957",
   "metadata": {},
   "source": [
    "Let's save the output as a pickle file to be used in Notebook 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "281d3db7-5bfa-4143-9d4f-db87f22870c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./resources/podcast_ready_data.pkl', 'wb') as file:\n",
    "    pickle.dump(save_string_pkl, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dccf336",
   "metadata": {},
   "source": [
    "### Next Notebook: TTS Workflow\n",
    "\n",
    "Now that we have our transcript ready, we are ready to generate the audio in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c7e456-497b-4080-8b52-6f399f9f8d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
