{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a1b5d7-fd98-4fa2-9bea-e68c514b9245",
   "metadata": {},
   "source": [
    "## Notebook 4: TTS Workflow\n",
    "\n",
    "We have the exact podcast transcripts ready now to generate our audio for the Podcast.\n",
    "\n",
    "In this notebook, we will learn how to generate Audio using both `suno/bark` and `parler-tts/parler-tts-mini-v1` models first. \n",
    "\n",
    "After that, we will use the output from Notebook 3 to generate our complete podcast\n",
    "\n",
    "Note: Please feel free to extend this notebook with newer models. The above two were chosen after some tests using a sample prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442758d-c48f-48ac-a4b0-558695290aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DEEPGRAM_API_KEY=f1e75452fbafeaaf1e4ee5a2b424e84336d0fb04\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "%env DEEPGRAM_API_KEY=YOUR_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd650176-ab17-47a7-8e02-10dc9ca9e852",
   "metadata": {},
   "source": [
    "## Bringing it together: Making the Podcast\n",
    "\n",
    "Okay now that we understand everything-we can now use the complete pipeline to generate the entire podcast\n",
    "\n",
    "Let's load in our pickle file from earlier and proceed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1dca30f-1226-4002-8e02-fd97e78ecc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./resources/podcast_ready_data.pkl', 'rb') as file:\n",
    "    PODCAST_TEXT = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4dbb3b3-cdd3-4a1f-a60a-661e64a67f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\n    (\"Speaker 1\", \"Welcome to today\\'s explosive episode of \\'AI Revolution Unleashed!\\' where we\\'re about to dive headfirst into the uncharted territories of knowledge distillation for Large Language Models. Today\\'s topic is literally a ticking time bomb that\\'s going to change the AI landscape forever. Think of it as the secret ingredient to unleashing the true potential of AI. So, fasten your seatbelt, and get ready for the wildest ride of your life!\"),\\n    (\"Speaker 2\", \"Umm, what exactly is knowledge distillation? I mean, I\\'ve heard of it before, but I\\'m not entirely sure what it does hm\"),\\n    (\"Speaker 1\", \"Ah, great question! Knowledge distillation is the process of transferring knowledge from a large, pre-trained model – think of it as the \\'teacher\\' – to a smaller, more efficient model – the \\'student.\\' This technique enables us to harness the power of massive language models without the need for astronomical computational resources and data. Essentially, it\\'s like downloading the brain of a super-intelligent being into a smaller, more nimble vessel s\"),\\n    (\"Speaker 2\", \"[laughs] That sounds like science fiction. Can you give me a simpler analogy, like, I don\\'t know, something I can relate to?\"),\\n    (\"Speaker 1\", \"Well, imagine you\\'re trying to learn a new language. You hire a tutor – the teacher – who\\'s fluent in the language, and you attend classes and learn from them. Over time, you become fluent in the language yourself, but with much fewer hours of study. That\\'s essentially what knowledge distillation does, but with AI models and large amounts of data.\"),\\n    (\"Speaker 2\", \"Hmm, that makes more sense. But how does it work in the AI world? I mean, what happens when the student model tries to mimic the teacher model?\" ),\\n    (\"Speaker 1\", \"The student model receives the teacher model\\'s softened softmax output, which is like a simplified version of the teacher\\'s knowledge. From there, the student uses various techniques like data augmentation, self-knowledge generation, and reinforcement learning to fine-tune its performance. It\\'s like the student is learning from the teacher\\'s recipes and experimenting with new ingredients to create its own dishes.\"),\\n    (\"Speaker 2\", \"Whoa, that\\'s quite a process. I had no idea AI models could learn like that [sigh]. Can you give me some examples of how knowledge distillation is being used in real-world applications?\"),\\n    (\"Speaker 1\", \" definitely. Companies like Meta use knowledge distillation to improve the performance of their conversational AI models. By transferring knowledge from a large pre-trained model to a smaller one, they can deploy AI-powered chatbots that are both efficient and accurate.\"),\\n    (\"Speaker 2\", \"Wow, that\\'s awesome! What about data augmentation? How does that fit into the picture?\"),\\n    (\"Speaker 1\", \"Data augmentation is the process of generating new data from existing data using various techniques like text manipulation, paraphrasing, and so on. This new data is then used to fine-tune the student model, enabling it to learn more about the task at hand. It\\'s like a chef creating new recipes based on existing ingredients – they\\'re experimenting with different flavors and techniques to create something new and exciting.\"),\\n    (\"Speaker 2\", \"Hmm, I never thought about AI using data augmentation like that [laughs].\"),\\n    (\"Speaker 1\", \"Exactly! And that\\'s what makes knowledge distillation so powerful. It\\'s not just about compressing large models into smaller ones; it\\'s about creating new and more efficient AI solutions that can tackle complex tasks in a variety of domains.\"),\\n    (\"Speaker 2\", \"Okay, I think I get it now. But what about the challenges? Are there any limitations to knowledge distillation?\"),\\n    (\"Speaker 1\", \"Of course, knowledge distillation is not without its challenges. For one, it can be computationally intensive, especially when working with large pre-trained models. There\\'s also the risk of losing some of the teacher\\'s knowledge during the distillation process. However, the benefits far outweigh the costs.\"),\\n    (\"Speaker 2\", \"That\\'s a great point. I never realized how much impact knowledge distillation could have on the AI landscape [sigh].\"),\\n    (\"Speaker 1\", \"Exactly! And that\\'s why today\\'s episode is so crucial. We\\'re on the cusp of a revolution in AI, and knowledge distillation is the key to unlocking its true potential. Stay tuned for more exciting episodes on the AI Revolution Unleashed podcast, and until next time, stay curious!\")\\n]'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PODCAST_TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b4c9e-379f-4004-bdd0-93a53f3f7ee0",
   "metadata": {},
   "source": [
    "Most of the times we argue in life that Data Structures isn't very useful. However, this time the knowledge comes in handy. \n",
    "\n",
    "We will take the string from the pickle file and load it in as a Tuple with the help of `ast.literal_eval()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9946e46c-3457-4bf9-9042-b89fa8f5b47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Speaker 1',\n",
       "  \"Welcome to today's explosive episode of 'AI Revolution Unleashed!' where we're about to dive headfirst into the uncharted territories of knowledge distillation for Large Language Models. Today's topic is literally a ticking time bomb that's going to change the AI landscape forever. Think of it as the secret ingredient to unleashing the true potential of AI. So, fasten your seatbelt, and get ready for the wildest ride of your life!\"),\n",
       " ('Speaker 2',\n",
       "  \"Umm, what exactly is knowledge distillation? I mean, I've heard of it before, but I'm not entirely sure what it does hm\"),\n",
       " ('Speaker 1',\n",
       "  \"Ah, great question! Knowledge distillation is the process of transferring knowledge from a large, pre-trained model – think of it as the 'teacher' – to a smaller, more efficient model – the 'student.' This technique enables us to harness the power of massive language models without the need for astronomical computational resources and data. Essentially, it's like downloading the brain of a super-intelligent being into a smaller, more nimble vessel s\"),\n",
       " ('Speaker 2',\n",
       "  \"[laughs] That sounds like science fiction. Can you give me a simpler analogy, like, I don't know, something I can relate to?\"),\n",
       " ('Speaker 1',\n",
       "  \"Well, imagine you're trying to learn a new language. You hire a tutor – the teacher – who's fluent in the language, and you attend classes and learn from them. Over time, you become fluent in the language yourself, but with much fewer hours of study. That's essentially what knowledge distillation does, but with AI models and large amounts of data.\"),\n",
       " ('Speaker 2',\n",
       "  'Hmm, that makes more sense. But how does it work in the AI world? I mean, what happens when the student model tries to mimic the teacher model?'),\n",
       " ('Speaker 1',\n",
       "  \"The student model receives the teacher model's softened softmax output, which is like a simplified version of the teacher's knowledge. From there, the student uses various techniques like data augmentation, self-knowledge generation, and reinforcement learning to fine-tune its performance. It's like the student is learning from the teacher's recipes and experimenting with new ingredients to create its own dishes.\"),\n",
       " ('Speaker 2',\n",
       "  \"Whoa, that's quite a process. I had no idea AI models could learn like that [sigh]. Can you give me some examples of how knowledge distillation is being used in real-world applications?\"),\n",
       " ('Speaker 1',\n",
       "  ' definitely. Companies like Meta use knowledge distillation to improve the performance of their conversational AI models. By transferring knowledge from a large pre-trained model to a smaller one, they can deploy AI-powered chatbots that are both efficient and accurate.'),\n",
       " ('Speaker 2',\n",
       "  \"Wow, that's awesome! What about data augmentation? How does that fit into the picture?\"),\n",
       " ('Speaker 1',\n",
       "  \"Data augmentation is the process of generating new data from existing data using various techniques like text manipulation, paraphrasing, and so on. This new data is then used to fine-tune the student model, enabling it to learn more about the task at hand. It's like a chef creating new recipes based on existing ingredients – they're experimenting with different flavors and techniques to create something new and exciting.\"),\n",
       " ('Speaker 2',\n",
       "  'Hmm, I never thought about AI using data augmentation like that [laughs].'),\n",
       " ('Speaker 1',\n",
       "  \"Exactly! And that's what makes knowledge distillation so powerful. It's not just about compressing large models into smaller ones; it's about creating new and more efficient AI solutions that can tackle complex tasks in a variety of domains.\"),\n",
       " ('Speaker 2',\n",
       "  'Okay, I think I get it now. But what about the challenges? Are there any limitations to knowledge distillation?'),\n",
       " ('Speaker 1',\n",
       "  \"Of course, knowledge distillation is not without its challenges. For one, it can be computationally intensive, especially when working with large pre-trained models. There's also the risk of losing some of the teacher's knowledge during the distillation process. However, the benefits far outweigh the costs.\"),\n",
       " ('Speaker 2',\n",
       "  \"That's a great point. I never realized how much impact knowledge distillation could have on the AI landscape [sigh].\"),\n",
       " ('Speaker 1',\n",
       "  \"Exactly! And that's why today's episode is so crucial. We're on the cusp of a revolution in AI, and knowledge distillation is the key to unlocking its true potential. Stay tuned for more exciting episodes on the AI Revolution Unleashed podcast, and until next time, stay curious!\")]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "ast.literal_eval(PODCAST_TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7b4c11-5526-4b13-b0a2-8ca541c475aa",
   "metadata": {},
   "source": [
    "#### Generating the Final Podcast with Deepgram Text to Speech\n",
    "\n",
    "Finally, we can loop over the Tuple and use our helper functions to generate the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b83c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEEPGRAM_API_KEY = os.environ[\"DEEPGRAM_API_KEY\"]\n",
    "OUTPUT_FOLDER = \"podcast_segments\"  # Folder to save audio segments\n",
    "VOICE_1 = \"aura-helios-en\"  # Speaker 1 voice model\n",
    "VOICE_2 = \"aura-asteria-en\"  # Speaker 2 voice model\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Function to call Deepgram TTS API\n",
    "def call_deepgram_tts(text, model):\n",
    "    url = f\"https://api.deepgram.com/v1/speak?model={model}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Token {DEEPGRAM_API_KEY}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"text\": text\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        raise Exception(f\"Deepgram TTS API error: {response.status_code} - {response.text}\")\n",
    "\n",
    "# Save each speaker's segment as an MP3 file\n",
    "def save_speaker_audio(text, model, filename):\n",
    "    audio_data = call_deepgram_tts(text, model)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(audio_data)\n",
    "\n",
    "# Function to call Deepgram TTS API\n",
    "def call_deepgram_tts(text, model):\n",
    "    url = f\"https://api.deepgram.com/v1/speak?model={model}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Token {DEEPGRAM_API_KEY}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"text\": text\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        raise Exception(f\"Deepgram TTS API error: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a84ea3d",
   "metadata": {},
   "source": [
    "## Combine all the mp3 files into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da8ca85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating podcast segments:   0%|          | 0/17 [00:00<?, ?segment/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating podcast segments: 100%|██████████| 17/17 [00:10<00:00,  1.68segment/s]\n"
     ]
    }
   ],
   "source": [
    "# Main code for generating and saving podcast segments\n",
    "for index, (speaker, text) in enumerate(tqdm(ast.literal_eval(PODCAST_TEXT), desc=\"Generating podcast segments\", unit=\"segment\")):\n",
    "    model = VOICE_1 if speaker == \"Speaker 1\" else VOICE_2\n",
    "    filename = os.path.join(OUTPUT_FOLDER, f\"segment_{index + 1}.mp3\")\n",
    "    save_speaker_audio(text, model, filename)\n",
    "\n",
    "# Combine all segments\n",
    "combined_audio = None\n",
    "for filename in sorted(os.listdir(OUTPUT_FOLDER)):\n",
    "    segment = AudioSegment.from_mp3(os.path.join(OUTPUT_FOLDER, filename))\n",
    "    if combined_audio is None:\n",
    "        combined_audio = segment\n",
    "    else:\n",
    "        combined_audio += segment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb2228-8023-44c4-aafe-d6e1d22ff8e4",
   "metadata": {},
   "source": [
    "### Output the Podcast\n",
    "\n",
    "We can now save this as a mp3 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2eeffdb7-875a-45ec-bdd8-c8c5b34f5a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='final_podcast.mp3'>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export the final combined audio\n",
    "combined_audio.export(\"final_podcast.mp3\", format=\"mp3\", bitrate=\"48k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce5836",
   "metadata": {},
   "source": [
    "### Suggested Next Steps:\n",
    "\n",
    "- Experiment with the prompts: Please feel free to experiment with the SYSTEM_PROMPT in the notebooks\n",
    "- Extend workflow beyond two speakers\n",
    "- Test other TTS Models\n",
    "- Experiment with Speech Enhancer models as a step 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "26cc56c5-b9c9-47c2-b860-0ea9f05c79af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
